\chapter{Introduction and Overview}
\citationChap{
    I never quite realized\ldots how beautiful this world is.
    }{A2 (\textit{NieR:Automata})}
\minitoc
\newpage

\section{Introduction}

Creativity is a long-standing goal of research in Artificial Intelligence (AI). The \textit{Proposal} \citep{mccarthy2006proposal} for the 1956 Dartmouth Summer Research Project on Artificial Intelligence---widely considered to be the founding event of AI as a field---lists creativity as one of its seven aspects of the ``AI problem'', surmising that the difference between creative thinking and unimaginative competent thinking lies in the injection of randomness. Indeed, creativity can be defined as ``the ability to generate novel, and valuable, ideas'' \citep{boden2009computer}, where \emph{valuable} can be understood as interesting, beautiful, useful, meaningful, elegant, etc. It is a complex process that involves exploring different possibilities and taking risks; it requires intuition, imagination, and emotional intelligence. It is therefore often argued that creativity is specific to human nature and that no computer could ever exhibit ``true'' creativity. Nevertheless, this has not deterred AI researchers from studying how computers could perform creative tasks. In particular, the automatic generation of stories has been a staple of the arduous quest for artificial creativity.

Stories have played an integral role in shaping human history and societies throughout time \citep{sinclair2005impact}. Oral traditions, myths, and legends have been passed down from generation to generation, providing a link between past and present. Stories are often used as a way to teach moral lessons, impart wisdom, and pass on practical skills. For example, many indigenous cultures use stories to teach children about the natural world, how to hunt and fish, and how to live sustainably. Similarly, folktales have been used throughout history to teach children important life skills such as respect for authority, honesty, and perseverance. In addition to preserving cultural heritage \citep{paolini2014storytelling} and transmitting knowledge \citep{kreiswirth2000merely}, stories provide entertainment and inspiration for people of all ages. They can be a source of joy, laughter, and emotional catharsis, helping individuals cope with the stresses and challenges of daily life. 

In recent years, research in cognitive sciences has shown that skillful storytelling enables a narrator to build an authentic connection with their audience and allows listeners to grasp complex concepts more intuitively \citep{suzuki2018dialogues}. Storytelling-based approaches are now used in multiple fields, such as:
\begin{enumerate}
    \item Communication: storytelling serves as a common ground for establishing a value-laden reality among diverse parties and help build a social relationship more quickly \citep{barker2010strategic};
    \item Education: stories can become a tool for asserting a life-enhancing value system, where knowledge can be shared in the public interest \citep{woodhouse2011storytelling};
    \item Gaming: storytelling allows for an experience that is modified in real time and tailored to the preferences of the player in order to provide a more personalized end result \citep{bostan2010interactive};
    \item Marketing: well-crafted storytelling may create positive associations with a consumer brand and increase consumers' willingness to pay for it \citep{lundqvist2013impact}.
\end{enumerate}

Since the 1960s, multiple approaches have been used to explore computational storytelling. Research in the field involves different aspects, \eg\ story understanding, story representation and story generation \citep{alabdulkarim2021automatic}. Furthermore, the recent development of transformer-based language models \citep{vaswani2017attention, devlin-etal-2019-bert} has led to the emergence of {\llmfull}s (\llm) \citep{brown2020language}, which have demonstrated impressive performance at many {\nlpfull} tasks. \autoref{fig:fake_introduction} shows an introduction to this thesis that has been automatically generated by an LLM called Mistral (more precisely, \texttt{Mistral-7B-Instruct-v0.1-GGUF}) and eloquently illustrates current language model capabilities. Most notably, Mistral produces text with impeccable fluency and, since it was given only the basic topic of the thesis, makes reasonable claims about its contents.

\begin{figure}[h!]
    \small
    \begin{framed}
    Automatic story generation (ASG) has seen rapid advancements in recent years, with large language models (LLMs) like BERT, RoBERTa, and GPT-3 demonstrating impressive performance. However, evaluating the quality of generated stories remains a challenging task due to subjectivity and lack of standardization. This calls for the development of robust meta-evaluation methodologies and benchmarks that can accurately assess ASG models' performance across various metrics.
    
    In this PhD thesis, we propose a comprehensive meta-evaluation framework for automatic story generation. Our approach combines multiple evaluation methods, including human assessment, expert judgment, and automated metrics, to provide a holistic understanding of model performance. We also introduce a novel benchmark dataset specifically designed for ASG, which includes diverse genres and narrative structures, enabling us to evaluate models' ability to generate coherent and engaging stories across various contexts.
    
    Our contributions include:

    \begin{enumerate}[nolistsep]
        \item  A comprehensive meta-evaluation framework that integrates multiple evaluation methods, providing a more accurate assessment of ASG model performance.
        \item A novel benchmark dataset specifically designed for automatic story generation, enabling us to evaluate models' ability to generate coherent and engaging stories across various contexts.
        \item An in-depth analysis of the strengths and weaknesses of different LLMs, shedding light on their suitability for specific ASG tasks and genres.
        \item A discussion on the importance of transparency and reproducibility in ASG research, emphasizing the need for open-source models and standardized evaluation methodologies.
    \end{enumerate}
    
    Through our work, we aim to advance the field of automatic story generation by providing a robust and comprehensive framework for model evaluation, enabling researchers to compare and improve their models more effectively.
    \end{framed}
    \caption{An introduction to this thesis generated by the Mistral language model with the following prompt: ``Write the introduction of a PhD thesis whose topic is `Meta-Evaluation Methodology and Benchmark for Automatic Story Generation'.''}
    \label{fig:fake_introduction}
\end{figure}

This setting, {\ie}, ``given a prompt, generate a story,'' will be the one used throughout our work.

\section{Research Questions}

In this section, we outline the four research questions that we address in this thesis.

\subsection{RQ1: How Can We Evaluate the Quality of a Story?}
While stories are central in human culture, determining whether a story is good or bad is fairly difficult. Compared to other {\nlpfull} (\nlp) tasks such as machine translation and text summarization, for which specific practices for evaluation have been designed, story evaluation is still an open question, especially due to its subjective nature. In \autoref{chap:methodology_story_evaluation}, we propose a methodology for story evaluation, including a comprehensive set of orthogonal criteria.

\subsection{RQ2: How Good Are Existing Models at Generating Stories?}
There have been many approaches to {\asgfull} (\asg), ever since the first systems were proposed in the 1960s \citep{ryan2017grimes, propp1968morphology}. However, many of those models produced stories that were still far from human performance. Recently, the development of neural networks has enabled language models to demonstrate impressive performance at many {\nlp} tasks, among which \asg. This naturally begs the question of whether automatically generated stories can compete with human-written ones. In \autoref{chap:hanna}, we design {\hanna}, a corpus of stories annotated with both human and {\llm} ratings, which allows us to compare existing systems with one another and with human writers.

\subsection{RQ3: To Which Extent Can We Use Automatic Measures for Story Evaluation?}
Human evaluation has always been the gold standard for the evaluation of any task in {\nlp}, but it is costly and time-consuming. To answer those issues, automatic evaluation measures were developed, such as {\bleu} \citep{papineni2002bleu} for machine translation. However, since there have been no automatic measure specifically designed for {\asefull} (\ase), researchers have been relying on measures that were introduced for other tasks. In \autoref{chap:meta_evaluation}, we perform an extensive meta-evaluation of {\asg} to assess whether existing automatic measures are appropriate for {\ase}, going from simple textual measures to {\llmfull}s used as automatic annotators.

\subsection{RQ4: How Explainable Are the Evaluation Ratings of {\llmfull}s?}
We find in \autoref{chap:meta_evaluation} that {\llm}s outperform other evaluation measures for {\ase}, and that they are currently the best proxy for human evaluation of stories. However {\llm}s are well-known for behaving in ways that are difficult to understand, and, as their usage becomes ever more prevalent, it is becoming increasingly important to assess how reliable they are. In \autoref{chap:llm_explainability}, we perform two experiments that aim at providing a better understanding of {\llm} performance at {\ase} and {\asg}: a three-part study on {\llm}-generated explanations, and an analysis of the influence of pretraining data on their {\asg} performance.

\section{Organization of This Thesis}

In this section, we provide a summary of each chapter of this thesis.

\paragraph{Chapter 2: Background.}
First, we provide some background information related to Automatic Story Generation and Evaluation ({\asg} and {\ase}). In \autoref{sec:preneural_asg}, we cover older methods for {\asg}: namely, structural and planning-based models. In \autoref{sec:language_models}, we present language models, focusing on neural-based language models, from recurrent neural networks to large language models. Finally, in \autoref{sec:eval_and_meta}, we present different aspects related to the question of evaluation in NLP.

\paragraph{Chapter 3: Methodology for Story Evaluation and {\asg} Meta-Evaluation.} 
In this chapter, we lay the groundwork for conducting a meta-evaluation of {\asgfull}. We begin by describing our chosen setting and providing definitions for the {\asg} and {\ase} tasks (\autoref{sec:methodology_introduction}) Then, we review the human evaluation methods used in the {\asg} literature (\autoref{sub:existing_human_evaluation}) and several approaches from the social sciences literature (\autoref{sub:social_sciences}). This allows us to propose an original, comprehensive set of six criteria (\autoref{sub:our_criteria}). We select and review a number of automatic evaluation measures designed for diverse {\nlp} tasks (\autoref{sub:survey_automatic_measures}) and grouped them in different classes {\wrt}\ a newly-proposed taxonomy (\autoref{sub:taxonomy_measures}). Finally, we describe our meta-evaluation framework, which we tailored for the {\asg} task (\autoref{sec:meta_evaluation_framework}).

\paragraph{Chapter 4: \hanna: A Corpus of Human-ANnotated NArratives for {\asg} Evaluation.}
In order to use our human evaluation criteria (\autoref{sec:human_criteria}) and to conduct a meta-evaluation with our proposed framework (\autoref{sec:meta_evaluation_framework}), we need an appropriate dataset of stories. In this chapter, we describe the building process of {\hanna}, our corpus of Human ANnotated NArratives. We begin by reviewing  corpora related to {\asgfull} and find that no existing dataset contains human annotations {\wrt}\ specific criteria of story quality (\autoref{sub:asg_corpora}). We then review and select {\asg} systems (\autoref{sub:neural_asg}) in order to build our own corpus, \hanna, specifically designed for {\asg} evaluation. The first version of {\hanna} contains 1,056 stories produced by 11 systems and aligned on 96 prompts (\autoref{sub:hanna_v1}). We perform an annotation experiment on {\hanna} V1, asking human raters to grade each story {\wrt}\ our six human criteria. We analyze the overall and system-level correlations between our human criteria, and confirm that they allow for a standardized and extensive human evaluation (\autoref{sub:evaluating_human_criteria}). We also observe that large pre-trained language models, namely {\gptt}, produce the best results for ASG (\autoref{sub:comparing_asg_systems}). Finally, we use {\llmfull}s to augment {\hanna} with 480 new stories (\autoref{sub:llm_methodology_asg}) and 150k+ annotations (\autoref{sub:llm_methodology_ase}), and observe that {\llm}s are rated more highly than humans when evaluated by two different {\llm}s (\autoref{sub:asg1_analysis}).

\paragraph{Chapter 5: Meta-Evaluation Benchmark of \asgfull.}
After we have described the methodology for our meta-evaluation of {\asg} (\autoref{chap:methodology_story_evaluation}) and built a corpus specifically designed for that task (\autoref{chap:hanna}), this chapter details our meta-evaluation of automatic evaluation measures for \asgfull. First, we present our meta-evaluation of non-LLM automatic measures (\autoref{sub:hanna_v1_measures}) on {\hanna} V1, and a more fine-grained analysis of measures using specific methods (\autoref{sub:fine_grained_analysis}). We mainly observe that specific measures for {\asefull} are needed, and that commonly used measures such as {\bleu} are sub-optimal. We then show our analysis of LLM performance at {\ase}: we compare LLMs to human and non-LLM automatic evaluation methods (\autoref{sub:ase1_analysis}) and we examine the influence of the Eval-Prompt on LLM performance (\autoref{sub:ase2_analysis}). We find that {\llm}s are currently the best proxy for human evaluation of {\asg} and that, in our specific setting, providing detailed guidelines does not improve correlations between {\llm} and human ratings.

\paragraph{Chapter 6: Exploring LLM Explainability for ASE and ASG.}
Since we observe in \autoref{sec:llms_for_ase} that {\llm}s correlate better with human judgment than other automatic measures, we wish to ascertain the reliability of our findings. In this chapter, we assess to which extent the performance displayed by {\llm}s at {\asefull} can be explained through different factors. We begin with a related work on model explainability (\autoref{sec:expl_related_work}). We perform two main experiments: a three-part study on LLM-generated explanations (\autoref{sec:ase3_explainability}), and an analysis of pretraining data on LLM performance (\autoref{sec:asg2_analysis}). For our analysis on {\llm} explanations, (1) we compute contextual embeddings for each explanation and perform a clustering analysis that shows that {\llm} explanations are overall well-separated {\wrt}\ our criteria (\autoref{sub:clustering_explanations}); (2) we perform a keyword analysis that confirms the previous findings (\autoref{sub:keyword_analysis}), and (3) we conduct a user study on LLM explanations to assess the appropriateness of the explanations (\autoref{sub:llm_user_study}). Notably, we find that {\llm}s struggle to explain their answers with substantiated claims. Next, in order to check for model contamination and to evaluate the influence of pretraining data on the {\asg} performance of LLMs, we use a detection method based on minimum token probabilities (\autoref{sec:asg2_analysis}). We find that the observed performance of {\llm}s in our experiments is not a result of model contamination, and that larger models tend to produce content that is more similar to that of existing books.

\paragraph{Chapter 7: Conclusion.} 
Finally, we summarize our main contributions and findings (\autoref{sec:ccl_contributions}), and outline three main research perspectives (\autoref{sec:ccl_perspectives}): designing specific {\asefull} measures, further investigating LLM performance at {\asg} and {\ase}, and assessing and mitigating the impact of LLMs on society.

\clearpage

\section{Publications}

This thesis has led to the publication of two research articles:

\begin{enumerate}
    \item \citep{chhun-etal-2022-human} \textbf{(\autoref{chap:methodology_story_evaluation}, \autoref{sec:hanna} and \autoref{sec:meta_evaluation_hanna_v1})}
    \begin{quote}
        \textbf{Cyril Chhun}, Pierre Colombo, Fabian M. Suchanek, and Chloé Clavel. 2022. Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation. In \textit{Proceedings of the 29th International Conference on Computational Linguistics}, pages 5794--5836, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.\\\url{https://aclanthology.org/2022.coling-1.509/}
    \end{quote}
    [Note: COLING was ranked A at the time of acceptance of this publication.]
    \item \citep{chhun2024do} \textbf{(\autoref{sec:hanna_v2}, \autoref{sec:llms_for_ase} and \autoref{chap:llm_explainability})}
    \begin{quote}
        \textbf{Cyril Chhun}, Fabian M. Suchanek, Chloé Clavel. 2024. Do Language Models Enjoy Their Own Stories? Prompting Large Language Models for Automatic Story Evaluation. \textit{Transactions of the Association for Computational Linguistics}, volume 12, pages 1122--1142.\\\url{https://doi.org/10.1162/tacl_a_00689}
    \end{quote}
\end{enumerate}

We also gave oral presentations for the NoRDF project\footnote{\url{https://nordf.telecom-paris.fr/}} and for the ``Journée Recherche TAL Défense'' at the DGA Maîtrise de l'information Center.

\begin{enumerate}
    \item \begin{quote}
        \textbf{Cyril Chhun}. April 19th, 2023. Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation. \textit{NoRDF Academic Workshop}, Télécom Paris, Palaiseau, France. Oral presentation.
    \end{quote}
    \item \begin{quote}
        \textbf{Cyril Chhun}. November 13th, 2023. Évaluation des histoires générées automatiquement. \textit{Journée Recherche TAL Défense}, DGA Maîtrise de l'information, Rennes, France. Oral presentation.
    \end{quote}
\end{enumerate}