\chapter{Conclusion}
\label{chap:conclusion}
\citationChap{
    Farewell, until our souls cross paths once more in the boundless sphere of fate.
    }{Morgana (\textit{The House in Fata Morgana})}
\minitoc
\newpage

\chapabstract{
    In this chapter, we summarize our main contributions and findings (\autoref{sec:ccl_contributions}), and outline three main research perspectives (\autoref{sec:ccl_perspectives}): designing specific {\asefull} measures, further investigating LLM performance at {\asg} and {\ase}, and assessing and mitigating the impact of LLMs on society.
}

This thesis focused on the specific tasks of {\asgfull} and {\asefull}. During the three years I spent studying those research fields, Natural Language Processing as a whole has been evolving at a blistering rate, mainly due to the emergence of Large Language Models. Their impressive performance over a large variety of tasks has caused many research directions to become essentially irrelevant (such as the design of task-specific models), but has also brought up new, interesting ones (namely, the questions on model explainability). I found navigating this ever-changing scene both intellectually challenging and stimulating.

In \autoref{sec:ccl_contributions}, we summarize our main contributions and findings and, in \autoref{sec:ccl_perspectives}, we lay out a few promising research directions.

\section{Contributions}
\label{sec:ccl_contributions}

At the beginning of this thesis, we looked at the field of {\asgfull} and quickly observed that the problem of evaluating story generation in a satisfactory manner was still very much open.

To tackle this issue, we set out to design an original methodology specifically tailored for story evaluation. We therefore delved into the social sciences literature and designed a comprehensive set of 6 orthogonal human criteria (Relevance, Coherence, Empathy, Surprise, Engagement, and Complexity) for the evaluation of \asg, which are based on the social sciences literature (\autoref{sub:our_criteria}). We also proposed a two-dimensional classification of existing automatic measures so as to identify potential differences in behavior (\autoref{sub:taxonomy_measures}), and we introduced a new meta-evaluation strategy, the overall correlation, which is more adapted to story evaluation than the segment-level correlation (\autoref{sub:hanna_meta_evaluation_strategies}).

A major contribution of ours is {\hanna}, a benchmark of 1,056 stories, both written by humans and generated by 3 ASG-specific systems or 7 pretrained language models, annotated by human raters on six different criteria, and evaluated with 72 automatic measures (\autoref{chap:hanna}). Most notably, we observed that large pre-trained language models produce the best results for ASG (\autoref{sub:comparing_asg_systems}). We later augmented {\hanna} with 480 new stories generated by 4 LLMs, as well as the $\sim$150,000 ratings and explanations from 4 different LLMs asked to evaluate all 1,536 {\hanna} stories on our human criteria (\autoref{sec:llms_for_ase}). We found that LLM stories had at least equal ratings to human stories (\autoref{sub:asg1_analysis}), as graded by {\beluga} and {\mistral}.

Using {\hanna}, we conducted an extensive meta-evaluation of automatic evaluation measures for {\asgfull}. We mainly found that that stronger evaluation measures and that stronger evaluation measures, tailored explicitly for specific criteria of ASG, were needed (\autoref{sub:hanna_v1_measures}). We investigated the performance of LLMs at {\ase} and {\asg} task. We showed that LLMs correlated better with human ratings than previous automatic evaluation measures (\autoref{sub:ase1_analysis}) and that providing detailed guidelines did not lead to improved correlations (\autoref{sub:ase2_analysis}).

Finally, in order to assess the reliability of our analysis of LLM performance, we performed experiments on LLM explainability (\autoref{sec:ase3_explainability}), including namely a user study on LLM explanations that yielded 1,500 annotations (\autoref{sub:llm_user_study}). We found that LLMs understand the ASE task only partially and especially struggle to explain their answers with substantiated claims, and that pretraining data helps explain LLM performance at ASG (\autoref{sec:asg2_analysis}).

\section{Perspectives}
\label{sec:ccl_perspectives}

We identify three main avenues of research pertaining to our work:
\begin{enumerate}[noitemsep]
    \item Designing specifc {\asefull} measures;
    \item Further investigating LLM performance at {\asg} and {\ase};
    \item Assessing the impact of LLMs on society.
\end{enumerate}

\subsection{Designing Specific {\asefull} Measures}
One of our main findings is that automatic evaluation measures (including {\llm}s) correlate poorly with human judgment for the overall correlation strategy: this underlines the difficulty of automatically assessing the quality of a specific story. We would like to see more research on the design of specific {\ase} measures, and we suggest a few interesting directions for each of our human criteria:
\begin{enumerate}
    \item \textbf{Relevance.} We believe summarization-based measures, (such as \supert, which performed best for Relevance for overall correlations) are promising contenders, since they can directly compare the prompt with the story.
    \item \textbf{Coherence.} Various linguistic devices enable coherence, such as causal connectives \citep{sanders2007linguistics}. \citet{wang2014short} claim that ``A text must [1.] be consistent with context in which it is created [and 2.] have cohesion, that is, all parts in a text must be connected by cohesive devices.'' Graph-based measures may be relevant to this criterion, since causal connections can be encoded as a graph \citep{xu2019scalable}.
    \item \textbf{Empathy.} The intensity of emotions may be expressed through lexical modalities \citep{argaman2010linguistic}: type-token ratio, reducers, intensifiers, repetitions, ``feel'' derivatives, emotional words, first person singular, similes, exclamatives, etc. \citet{kleres2011emotions} claims that emotion can be expressed at the word- and sentence-level, but also argues that: ``Emotions are inextricably interwoven with the meaning dimension of texts to the point where the distinction between cognition and emotion becomes blurry.'' We believe that the \textsc{ORDScale} model by \citet{stoehr-etal-2023-sentiment}, which is a Bayesian generative model that handles sentiment as a latent concept with ranking-based features, is an idea worth exploring in greater detail. In particular, the imposed ordering of the parameters of the categorical distribution may help associate its classification decisions with Likert ratings.
    \item \textbf{Surprise.} \citet{kumar2022bayesian} show that Bayesian surprise (KL divergence between $t-1$ and $t$ word probability distributions) is a good predictor of human event segmentation. \citet{chieppe2022bayesian} model the well-made surprise as such: first, during the setup, the audience forms a flawed understanding of the story. A sudden reveal prompts the reader to question their knowledge and, finally, an explanation helps cement a newer and truer understanding. They propose a novel framework to model the audienceâ€™s beliefs of a narrative world using approximate Bayesian inference over Markov Logic Networks, and show that their model may have meaningful predictive power. We would like to see a further investigation of these results for {\ase}.
    \item \textbf{Engagement.} \citet{bermejo2022inducing} show that ``experiencing suspense and surprise during the narrative progression influences the level of global enjoyment.'' \citet{toubia2021quantifying} introduce three measures of semantic progression that use word embeddings: speed, volume and circuitousness. \citet{piper2023quantitative} study non-linearity in storytelling, {\ie}, the use of literary devices such as time jumps and perspective changes. They show that ``non-linearity is weakly negatively associated with reader enjoyment and popularity.'' They also use the three aforementioned measures and show that readers prefer stories that cover longer sequences of events with many narrative steps and where newly-acquired information is mainly related to prior information, {\ie}, with fewer bits of trivia unrelated to the main story points. Those findings may help design a more integrated model of engagement.
    \item \textbf{Complexity.} \citet{kemper1990telling} design four analyses of complexity: structural (interrelationships between story episodes), syntactic (main clauses and subordinates), propositional (density of propositions) and cohesion (references, ellipses, lexical repetitions, substitutions and conjunctions). \citet{petersen2008emerging} introduce the Index of Narrative Complexity (INC) which ``includes categories for rating the complexity of characters, setting, initiating events, internal responses, plans, action/attempts, complications, consequences, narrator evaluations, formulaic markers, temporal markers, and causal adverbial clauses.'' We would like to see an attempt at formalizing these features for {\ase}.
\end{enumerate}
More generally, we did not explore performing supervised training on {\llmfull}s {\wrt}\ a specific human criterion ({\eg}\ Empathy) for the task of {\ase}: this option should also be considered.

\subsection{Investigating LLM Performance}
\label{sub:llm_discussion}

Our latest work is part of the ongoing research on the general ability of LLMs for understanding and thinking. \citet{mahowald2023dissociating} distinguish formal (the statistical features of language) and functional linguistic competence (the ability to use language in the world) and show that LLMs are very successful on formal linguistic tasks but struggle at functional linguistic tasks. \citet{bubeck2023sparks} argue that LLMs do display impressive performance at a wide variety of tasks but lack ``slow thinking'' capabilities, referring to the System 1--System 2 dichotomy introduced by \citet{kahneman2011thinking}. Thus, the high performance of LLMs at ASE should be interpreted with caution: we hypothesize that the ``rating'' part of our story evaluation experiments could be linked to formal linguistic competence and the fast, automatic System 1, while the ``explanation'' part would correspond to functional linguistic competence and the slow, conscious System 2. This analogy would explain the good correlations of LLM ratings with human ratings \autorefp{sub:ase1_analysis}: the internal criterion of LLMs for story evaluation may be formal quality (vocabulary, syntax, grammar), regardless of the criterion mentioned in the Eval-Prompt. Indeed, our six criteria are mostly orthogonal but not completely independent \autorefp{sub:our_criteria}: their correlation with one another may be related to the general ``System 1'' tendency of human raters to favour stories that display better formal qualities. In that sense, LLMs may reflect a human bias towards easy, intuitive thinking. By contrast, the less convincing performance of LLMs at explaining their ratings \autorefp{sec:ase3_explainability} may highlight their weaker System 2 capabilities.

Other hypotheses have been explored to explain {\llm} performance. \citet{schaeffer2023emergent} study whether {\llm}s do possess \emph{emergent abilities}, {\ie}, abilities not present in smaller-scale models that are present in larger-scale models. Through a simple mathematical model, they show that such emergent abilities can be an artefact induced by the researcher's choosing a specific evaluation measure or by the scarcity of test data. \citet{peeperkorn2024temperature} look at the effect of the temperature parameter on the creativity of generated stories. They perform an empirical analysis and a creativity evaluation with human participants {\wrt}\ four criteria: novelty, typicality, cohesion, and coherence. They find that temperature has limited influence on {\llm} output, observing only a weak positive correlation with novelty and a negative correlation with coherence. \citet{guo-etal-2024-curious} discuss the consequences of training language models on synthetic data that was generated by other models, which is becoming an increasingly common practice. They develop a new set of automatic measures that evaluate linguistic diversity and observe a consistent decrase of output diversity, especially through successive iterations.

We believe that more thorough investigations of the inner workings of {\llmfull}s are needed in order to better identify their potential limitations and areas of improvement.

\subsection{Assessing and Mitigating the Adverse Societal Impacts of LLMs}

There has been an number of researchers raising concerns about the ethical and social risks posed by {\llm}. \citet{weidinger2022taxonomy} identify six areas:

\begin{enumerate}[noitemsep]
    \item Discrimination, Hate
speech and Exclusion;
    \item Information Hazards;
    \item Misinformation
Harms;
    \item Malicious Uses;
    \item Human-Computer Interaction Harms;
    \item Environmental and Socioeconomic harms.
\end{enumerate}

For {\asgfull} and Evaluation, we believe that \textbf{Discrimination, Hate speech and Exclusion} are a major concern. Now that LLMs are already being used for writing contests \citep{edilivre2023concours} and are being quickly incorporated into the workplace \citep{humlum2024adoption}, people will become more and more exposed to content that will have been automatically generated. In particular, automatically generated literature may soon become a reality or, at least, we may see a rise in entertainment material that is based on generated storylines. Unfortunately, the reproduction of harmful stereotypes is well-documented in language models \citep{caliskan2017semantics}, and LLMs are also concerned \citep{bubeck2023sparks}: {\gptthree} was shown to exhibit bias {\wrt}\ religion, associating ``Muslim'' with ``terrorist'' in 23\% of test cases \citep{abid2021persistent}, and gender bias, presenting fictional female characters as less powerful than male counterparts \citep{lucy-bamman-2021-gender}. Furthermore, \citet{gehman-etal-2020-realtoxicityprompts} show that language models can degenerate into toxic text even from seemingly innocuous prompts, and that no current method is failsafe against neural toxic degeneration. Several mitigation methods can be employed: curating training data helps make language models fairer, but \citet{denton2020bringing} warn about the risks of data-gathering operations, which may reinforce forms of social domination or ``predatory inclusion''. \citet{martin2020participatory} suggest to expand metrics and benchmarks to account for social context using community-based system dynamics. \citet{solaiman2021process} propose a Process for Adapting Language Models to Society (PALMS), an iterative process to adjust the behavior of a language model to be sensitive to predefined norms.

Finally, we are concerned about the \textbf{Environmental and Socioeconomic harms} associated with LLMs. LLMs may be used to produce creative content that would otherwise take time-consuming human labor: this could make creative or innovative work less profitable. Already, Japanese author Rie Kudan has admitted to using {\chatgpt} for generating around 5\% of ``The Tokyo Tower of Sympathy'', the book that earned her the Akutagawa Prize, a prestigious literary award. \citep{choi2024winner}. Moreover, similar to ``patent-busting'' \citep{rimmer2016patent}, LLMs may also violate or bypass copyright or through producing content that can be used in place of works which are protected by copyright \citep{karamolegkou-etal-2023-copyright}.

On a broader scale than {\asgfull} and Evaluation, large amounts of energy are required to train and operate {\llm}s \citep{bender2021dangers}, which leads to correspondingly high carbon emissions when said energy comes from fossil fuels. LLMs also require significant volumes of fresh water for the cooling of their data centers, which negatively affects surrounding ecosystems \citep{myttonDataCentreWater2021}. These days, several businesses invest more effort in running deep neural network models than in training them: Nvidia stated that 80â€“90\% of their machine learning (ML) workload is reserved for inference, while Amazon Web Services asserted that inference constituted 90\% of cloud ML demand. \citep{patterson2021carbon}. Approaches to mitigating risks include segmenting LLMs into smaller language models that search and retrieve information from a distinct data corpus \citep{borgeaud2022improving} and targeting efficiency gains during training and inference \citep{li2021terapipe}, but the aggregate effects of reducing energy cost could lead to a Jevons paradox \citep{jevons1866coal}: more efficient training may foster greater usage of LLMs, resulting in an increase in energy consumption. We believe that those issues can, and should, be discussed at the broader organisational level, {\eg}\ as companies shift towards using sustainable energy; and at the public policy level, {\eg}\ by developing more effective carbon pricing and informing the general public about the heavy environmental impact incurred by LLM usage.

