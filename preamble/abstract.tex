\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

Storytelling is a central component of human culture: it shapes societies through the transmission of shared traditions and common knowledge. Multiple approaches have been proposed to explore computational storytelling, despite the inherent challenges posed by the tasks of generating stories and assessing their quality. In this thesis, we design a meta-evaluation methodology and benchmark for {\asgfull} (\asg).

First, we lay the groundwork for conducting our meta-evaluation: we describe our chosen setting, provide definitions for the {\asg} and {\asefull} (\ase) tasks, and review the {\asg} and social sciences literature to propose an original, comprehensive set of six criteria for story evaluation: Relevance, Coherence, Empathy, Surprise, Engagement, and Complexity. We also review and select multiple automatic measures and group them {\wrt}\ a newly-proposed, two-dimensional taxonomy. Finally, we introduce our meta-evaluation framework, which we adapted specifically to {\asg} through the design of a new meta-evaluation strategy: the overall correlation.

Then, in order to use our human evaluation criteria and to conduct a meta-evaluation with our proposed framework, we introduce {\hanna}, our corpus of Human ANnotated NArratives. We begin by reviewing corpora related to {\asg} and find that no existing dataset contains human annotations w.r.t. specific criteria of story quality. We select several {\asg} systems to build {\hanna}, which contains 1,056 stories produced by 11 different systems, aligned on 96 prompts, and annotated {\wrt}\ our six criteria. We analyze the overall and system-level correlations between our criteria, showing that they allow for a standardized human evaluation. We also observe that large pre-trained language models such as {\gptt} produce the best results for {\asg}. We use {\llmfull}s ({\llm}s) to augment {\hanna} with 480 new stories and conduct an annotation experiment with {\llm}s which allows us to collect 150k+ annotations of story ratings {\wrt}\ different Eval-Prompts. Mainly, we observe that {\llm}s obtain better grades than humans, as rated by our selected {\llm}s: {\beluga} and {\mistral}.

After that, we perform our meta-evaluation benchmark of automatic measures on {\hanna}, where we study their overall and system-level correlations with human ratings through different strategies, which we complete with more fine-grained analyses: an experiment on the correlations of top-$k$ systems, a pairwise system comparison with bootstrap resampling, and a discussion on the statistical discernibility of our results. We mainly observe that overall correlations of automatic measures with human criteria remain weak, meaning that specific measures for {\ase} are needed, and that commonly-used measures such as {\bleu} are sub-optimal compared with other existing measures ({\eg}\ {\bertscore} and {\bartscore}). We then show our analysis of {\llm} performance at {\ase}: we compare {\llm}s to human and non-{\llm} automatic evaluation methods and we examine the influence of the Eval-Prompt on {\llm} performance. We find that {\llm}s are currently the best proxy for human evaluation of {\asg}, especially {\wrt}\ system-level correlations, that {\llm}s are remarkably self-consistent, and that, in our specific setting, providing detailed guidelines does not improve correlations between {\llm} and human ratings.

Our results prompt us to study whether the performance displayed by {\llm}s at {\ase} and {\asg} can be explained through different factors. We perform two main experiments: a three-part study on {\llm}-generated explanations, and an analysis of pretraining data on {\llm} performance. For the former, (1) we compute contextual embeddings for each {\llm} explanation in order to perform a clustering analysis and show that explanations are well-separated {\wrt}\ our criteria, (2) we perform a keyword analysis that confirms the previous findings, and (3) we conduct a user study to assess the appropriateness of {\llm} explanations. Notably, we find that {\llm}s struggle to explain their answers with substantiated claims. For the latter, we use a detection method based on minimum token probabilities to show that larger models tend to produce content that is more similar to that of existing books.

To conclude, we outline three main research perspectives: designing specific {\ase} measures, further investigating {\llm} performance at {\asg} and {\ase}, and assessing and mitigating the impact of {\llm}s on society.