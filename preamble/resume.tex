\chapter*{\texorpdfstring{\foreignlanguage{french}{Résumé}}{Résumé}}
\addcontentsline{toc}{chapter}{\texorpdfstring{\foreignlanguage{french}{Résumé}}{Résumé}}

\begin{otherlanguage}{french}

La narration d'histoires est un élément central de la culture humaine : elle façonne les sociétés par la transmission de traditions partagées et de connaissances communes. De nombreuses approches de narration numérique ont été proposées, malgré les défis inhérents aux tâches de génération d'histoires et d'évaluation de leur qualité. Dans cette thèse, nous concevons une méthodologie et un benchmark pour la méta-évaluation de la génération automatique d'histoires (\asg).

Tout d'abord, nous posons les bases de notre méta-évaluation : nous décrivons le contexte choisi, définissons les tâches d'{\asg} et d'évaluation automatique d'histoires ({\ase}), et passons en revue les littératures d'{\asg} et des sciences sociales afin de proposer un ensemble original de six critères pour l'évaluation d'histoires : Pertinence, Cohérence, Empathie, Surprise, Engagement et Complexité. Nous examinons et sélectionnons également de multiples mesures automatiques et les regroupons dans une taxonomie bidimensionnelle nouvellement proposée. Enfin, nous présentons notre cadre de méta-évaluation, que nous avons adapté spécifiquement à l'{\asg} en concevant une nouvelle stratégie de méta-évaluation : la corrélation globale.

Ensuite, afin d'utiliser nos critères d'évaluation humaine et de mener une méta-évaluation avec le cadre proposé, nous présentons {\hanna}, notre corpus d'histoires annotées manuellement (Human ANnotated NArratives). Nous commençons par passer en revue les corpus liés à l'{\asg} et constatons qu'aucun ensemble de données existant ne contient d'annotations humaines en fonction de critères spécifiques de qualité des histoires. Nous sélectionnons plusieurs systèmes d'{\asg} pour construire {\hanna}, qui contient 1 056 histoires produites par 11 systèmes différents, alignées sur 96 prompts, et annotées selon nos six critères. Nous analysons les corrélations globales et au niveau du système entre nos critères pour montrer que ces derniers permettent une évaluation humaine standardisée. Nous observons également que les modèles linguistiques pré-entraînés de plus grande taille tels que {\gptt} produisent les meilleurs résultats pour l'{\asg}. Nous utilisons des grands modèles de langage ({\llm}) pour ajouter 480 nouvelles histoires à {\hanna} et nous menons une expérience d'annotation avec des {\llm} qui nous permet de collecter 150k+ annotations de notes d'histoires en fonction de différents prompts d'évaluation. Nous observons principalement que les {\llm} obtiennent de meilleures notes que les humains, tels que notés par les {\llm} que nous avons sélectionnés : {\beluga} et {\mistral}.

Ensuite, nous effectuons notre méta-évaluation comparative des mesures automatiques sur {\hanna}, où nous étudions leurs corrélations globales et au niveau du système avec les évaluations humaines par le biais de différentes stratégies, que nous complétons avec des analyses plus minutieuses : une expérience sur les corrélations des systèmes top-$k$, une comparaison des systèmes par paires avec rééchantillonnage bootstrap, et une discussion sur la discernabilité statistique de nos résultats. Nous observons principalement que les corrélations globales des mesures automatiques avec les critères humains demeurent faibles, ce qui souligne la nécessité de concevoir des mesures spécifiques pour l'{\ase}, et que les mesures couramment utilisées telles que {\bleu} sont sous-optimales par rapport à d'autres mesures existantes comme {\bertscore} et {\bartscore}. Nous montrons ensuite notre analyse de la performance des {\llm} pour l'{\ase} : nous comparons les {\llm} aux méthodes d'évaluation automatique humaines et non-{\llm} et nous examinons l'influence du prompt d'évaluation sur la performance des {\llm}. Nous constatons que les {\llm} sont actuellement la meilleure approximation de l'évaluation humaine de l'{\asg}, en particulier en termes de corrélations au niveau du système, que les {\llm} sont remarquablement cohérents avec eux-mêmes, et que, dans notre contexte spécifique, fournir des consignes détaillées n'améliore pas les corrélations entre les {\llm} et les évaluations humaines.

Nos résultats nous incitent à étudier si les performances affichées par les {\llm} en {\ase} et {\asg} peuvent être expliquées par différents facteurs. Nous réalisons deux expériences principales : une étude en trois parties sur les explications générées par les {\llm} et une analyse des données de pré-entraînement sur les performance des {\llm}. Dans le premier cas, (1) nous construisons des plongements contextuels pour chaque explication de {\llm} afin d'effectuer une analyse de partitionnement : nous montrons ainsi que les explications sont correctement séparées en fonction de nos critères ; (2) nous effectuons une analyse de mots-clés qui confirme les résultats précédents, et (3) nous menons une étude utilisateur pour évaluer l'adéquation des explications {\llm}. Notamment, nous constatons que les {\llm} éprouvent des difficultés à expliquer leurs réponses par des affirmations étayées. Dans le second cas, nous utilisons une méthode de détection fondée sur les probabilités minimales de jeton pour montrer que les modèles plus importants ont tendance à produire un contenu plus similaire à celui des livres existants.

Pour conclure, nous présentons trois perspectives de recherche principales : la conception de mesures spécifiques à l'{\ase}, l'étude plus approfondie des performances des {\llm} en {\asg} et {\ase}, et l'évaluation et l'atténuation des risques des {\llm} sur la société.

\end{otherlanguage}